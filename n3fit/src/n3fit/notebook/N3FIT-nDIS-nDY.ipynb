{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c613114c",
   "metadata": {},
   "source": [
    "# Extend N3FIT to integrate nDIS/nDY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d11b87",
   "metadata": {},
   "source": [
    "The following notebook replicates the main parts of the **n3fit** fitting code. Its main purpose is to serve as a playground to implement various (subtle) features such as the inclusion of nuclear fits. Similar to the main fitting code, we rely on the `n3fit.backends` to perform various operations. As an important information, this notebook also relies on a modified version of some parts of the main code, these changes mainly affect the `backends` and the `layers` modules. Incrementally, we are going to complete the **nDIS** part first and then the **nDY**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1880c2",
   "metadata": {},
   "source": [
    "## 1. Short introduction\n",
    "\n",
    "Most of the nuclear (Neutral Current, or in short NC) DIS datasets are given as ration of structure functions with different nuclei:\n",
    "$$ \\mathcal{O} (x, A_1, A_2, Q^2) = \\frac{F_2 (x, A_1, Q^2)}{F_2 (x, A_2, Q^2)} \\quad \\mathrm{with} \\quad F_2 (x, A, Q^2) = \\sum^{n_f}_{i} \\sum^{n_x}_{\\alpha} \\mathrm{FK}_{ij} (x, x_\\alpha, Q^2, Q^2_0) f_i^A (x, Q^2_0) $$\n",
    "where $A$ denotes the atomic mass number, $f^A$ denotes the bound-nucleon PDF for a nucleus with atomic number $A$, and the rest carries the usual meaning. It is important to emphasize that the **FK** tables that appear in the numerator and in the denominator are the same. In turns, the bound-nucleon PDFs $f_i^A$ at a momentum fraction $x$ and scale $Q^2_0$ are expressed in terms of the bound-proton PDFs $f_i^{p/A}$ and bound-neutron PDFs $f_i^{n/A}$ as follows:\n",
    "$$ f_i^A (x, Q^2_0) = Z f_i^{p/A} (x, Q^2_0) + (A-Z) f_i^{n/A} (x, Q^2_0). $$\n",
    "The bound-proton and bound-neutron PDFs are related by **isospin asymmetry** via the following relations:\n",
    "$$ u^{p/A}(x, Q^2_0) = d^{n/A}(x, Q^2_0), \\: d^{p/A}(x, Q^2_0) = u^{n/A}(x, Q^2_0), \\: \\bar{u}^{p/A}(x, Q^2_0) = \\bar{d}^{n/A}(x, Q^2_0), \\: \\bar{d}^{p/A}(x, Q^2_0) = \\bar{u}^{n/A}(x, Q^2_0) $$\n",
    "and $f_i^{p/A} = f_i^{n/A}$ for other PDF flavours. In practice, one fits the bound-proton PDFs in which constraints such as **sum rules** can be imposed.\n",
    "\n",
    "#### Evolution basis version:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef10b8",
   "metadata": {},
   "source": [
    "## 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc6bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras backend\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from dataclasses import dataclass\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Use n3fit backends which are wrappers around\n",
    "# tf.keras backends.\n",
    "from n3fit.backends import Input\n",
    "from n3fit.backends import base_layer_selector\n",
    "from n3fit.backends import MetaModel\n",
    "from n3fit.backends import callbacks\n",
    "from n3fit.backends import MetaLayer\n",
    "from n3fit.backends import operations as op\n",
    "from n3fit.backends import clear_backend_state\n",
    "\n",
    "from n3fit.stopping import Stopping\n",
    "from n3fit.msr import msr_impose\n",
    "\n",
    "from n3fit.layers import DIS, DY\n",
    "from n3fit.layers import ObsRotation\n",
    "from n3fit.layers import losses\n",
    "from n3fit.layers import Preprocessing\n",
    "from n3fit.layers import FkRotation\n",
    "from n3fit.layers import FlavourToEvolution\n",
    "from n3fit.backends import MetaLayer, Lambda\n",
    "from n3fit.backends import base_layer_selector\n",
    "from n3fit.backends import regularizer_selector\n",
    "\n",
    "# Define seeds\n",
    "random.seed(123)\n",
    "np.random.seed(456)\n",
    "console = Console()\n",
    "\n",
    "from n3fit.vpinterface import N3PDF\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca600c",
   "metadata": {},
   "source": [
    "## 3. Load toy-datasets and Add $A$-dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b76845",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we use as inputs saved files (`toyexpinfo.pkl` for experimental datasets, `posdatasets.pkl` for positivity datasets, and `integdatasets.pkl` for integrability datasets) generated from a **n3fit** run using the `toy-runcard.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ce68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pkl_file = open(\"toyexpinfo.pkl\", \"rb\")\n",
    "toyexpinfo = pickle.load(exp_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f834749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pkl_file = open(\"posdatasets.pkl\", \"rb\")\n",
    "toyposdatasets = pickle.load(pos_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1f4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "integ_pkl_file = open(\"integdatasets.pkl\", \"rb\")\n",
    "toyintegdatasets = pickle.load(integ_pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375b872",
   "metadata": {},
   "source": [
    "Now, we need to **add** the $A$-dependence to the various datasets to be passed along the FK tables for training. One still needs to think about how to include such information in **n3fit/validphys**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3295cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dicts = {\"PB\": 208, \"BE\": 9, \"C\": 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ebdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_A_dependence(expinfo: list) -> list:\n",
    "    \"\"\"Takes the usual inputs for n3fit (raw datasets from validphys/NNPDF)\n",
    "    and add the A-values to the dictionaries. The following should be added\n",
    "    to validphys/n3fit somehow, for examples, through the input run card.\n",
    "    \n",
    "    This is just for testing purposes, ie. not physical meanings at all.\n",
    "    \"\"\"\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            name_split = dataset[\"name\"].split(\"_\")\n",
    "            if len(name_split) <= 2:\n",
    "                A1, A2 = 1, 1\n",
    "            elif len(name_split) == 3:\n",
    "                A1 = A_dicts[name_split[1]]\n",
    "                A2 = A_dicts[name_split[2]]\n",
    "            else:\n",
    "                raise ValueError(\"Unappropriate Dataset\")\n",
    "            dataset[\"A1\"] = A1\n",
    "            dataset[\"A2\"] = A2\n",
    "    return expinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc6c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_active_A(expinfo: list) -> list:\n",
    "    \"\"\"Take the new list from `add_A_dependence` in which the information\n",
    "    on the atomic mass number A is included and returns an order list of\n",
    "    the A included in the fit.\n",
    "    \"\"\"\n",
    "    A_lists = []\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            A_lists.append(dataset[\"A1\"])\n",
    "            A_lists.append(dataset[\"A2\"])\n",
    "    return sorted(list(set(A_lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac890c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_expinfo(expinfo: list) -> None:\n",
    "    \"\"\"Summarize the information concerning the atomic mass number\n",
    "    for the datasets included in the actual fitting playgrounds.\n",
    "    \"\"\"\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Dataset\", justify=\"left\", width=24)\n",
    "    table.add_column(\"A1\", justify=\"left\", width=24)\n",
    "    table.add_column(\"A2\", justify=\"left\", width=24)\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            table.add_row(\n",
    "                f\"{dataset['name']}\", \n",
    "                f\"{dataset['A1']}\", \n",
    "                f\"{dataset['A2']}\"\n",
    "            )\n",
    "    console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca43601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_toyexpinfo = add_A_dependence(toyexpinfo)\n",
    "list_fitted_A = list_active_A(new_toyexpinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbd5a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Dataset                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> A1                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> A2                       </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ NMCPD_dw                 │ 1                        │ 1                        │\n",
       "│ SLACP_dwsh               │ 1                        │ 1                        │\n",
       "│ NMC_PB_C                 │ 208                      │ 12                       │\n",
       "│ NMC_BE_C                 │ 9                        │ 12                       │\n",
       "└──────────────────────────┴──────────────────────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mDataset                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mA1                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mA2                      \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ NMCPD_dw                 │ 1                        │ 1                        │\n",
       "│ SLACP_dwsh               │ 1                        │ 1                        │\n",
       "│ NMC_PB_C                 │ 208                      │ 12                       │\n",
       "│ NMC_BE_C                 │ 9                        │ 12                       │\n",
       "└──────────────────────────┴──────────────────────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">List of fitted As: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">208</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mList of fitted As: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m208\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_expinfo(new_toyexpinfo)\n",
    "console.print(f\"List of fitted As: {list_fitted_A}\", style=\"bold cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62078757",
   "metadata": {},
   "source": [
    "## 4. Create the NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464fef9",
   "metadata": {},
   "source": [
    "### 4.1 Construct the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7e0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_network(\n",
    "    nodes_in,\n",
    "    nodes,\n",
    "    activations,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    As_number=1,\n",
    "    seed=0,\n",
    "    dropout_rate=0.0,\n",
    "    regularizer=None,\n",
    "):\n",
    "    \"\"\"This function generate the different `tf.keras.layers.Dense` layers and add\n",
    "    them to a list. The number of layers depends on the length of the `node_per_layer`.\n",
    "    This function mimicks the behaviour of the `generate_dense_network` function in \n",
    "    the `n3fit.model_gen` module.\n",
    "    \n",
    "    Modifications:\n",
    "    --------------\n",
    "    For fits in the evolution basis, the output of the NNs are 8 PDFs, all for the\n",
    "    proton PDFs. When including the nuclear PDFs, this number is multiplied by the\n",
    "    number of active A's involved in the fit.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        As_number: int\n",
    "            Number of active A's (nb of A involved in the fit.)\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "        list:\n",
    "            List of `tf.keras.layers.Dense` layers of dimension `nodes[-1]*As_number`\n",
    "    \"\"\"\n",
    "    list_of_pdf_layers = []\n",
    "    # Modifications: Multiply the number of nodes in the last layer\n",
    "    # with the number of active As involved in the fitting procedure.\n",
    "    nodes[-1] *= As_number\n",
    "    number_of_layers = len(nodes)\n",
    "    if dropout_rate > 0:\n",
    "        dropout_layer = number_of_layers - 2\n",
    "    else:\n",
    "        dropout_layer = -1\n",
    "    for i, (nodes_out, activation) in enumerate(zip(nodes, activations)):\n",
    "        if dropout_rate > 0 and i == dropout_layer:\n",
    "            list_of_pdf_layers.append(base_layer_selector(\"dropout\", rate=dropout_rate))\n",
    "        init = MetaLayer.select_initializer(initializer_name, seed=seed + i)\n",
    "        arguments = {\n",
    "            \"kernel_initializer\": init,\n",
    "            \"units\": int(nodes_out),\n",
    "            \"activation\": activation,\n",
    "            \"input_shape\": (nodes_in,),\n",
    "            \"kernel_regularizer\": regularizer,\n",
    "        }\n",
    "        layer = base_layer_selector(\"dense\", **arguments)\n",
    "        list_of_pdf_layers.append(layer)\n",
    "        nodes_in = int(nodes_out)\n",
    "\n",
    "    return list_of_pdf_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc92da",
   "metadata": {},
   "source": [
    "### 4.3 Construct the complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da632528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfNN_layer_generator(\n",
    "    inp=2,\n",
    "    nodes=None,\n",
    "    activations=None,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    As_number=1,\n",
    "    layer_type=\"dense\",\n",
    "    flav_info=None,\n",
    "    fitbasis=\"NN31IC\",\n",
    "    out=14,\n",
    "    seed=None,\n",
    "    dropout=0.0,\n",
    "    regularizer=None,\n",
    "    regularizer_args=None,\n",
    "    impose_sumrule=None,\n",
    "    scaler=None,\n",
    "    parallel_models=1,\n",
    "):\n",
    "    \"\"\"In case of proton fit, this function acts in the standard way. In case A!=1,\n",
    "    further extensions had to be implemented. Recall that the output of the\n",
    "    `generate_dens_network` has a dimension (FITTING_BASIS_SIZE*As_number). Similar\n",
    "    operations as in the proton fit therefore applies to each individual A involved \n",
    "    in the fit. As a result, various custom layers had to be extended to take this\n",
    "    into consideration.\n",
    "    \n",
    "        * Modified layers so far: FKRotation, FlavourToEvolution, msr_impose\n",
    "        * Stil needs to be modified: preprocessing\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = parallel_models * [None]\n",
    "    elif isinstance(seed, int):\n",
    "        seed = parallel_models * [seed]\n",
    "\n",
    "    if nodes is None:\n",
    "        nodes = [15, 8]\n",
    "    ln = len(nodes)\n",
    "\n",
    "    if impose_sumrule is None:\n",
    "        impose_sumrule = \"All\"\n",
    "\n",
    "    if scaler:\n",
    "        inp = 1\n",
    "\n",
    "    if activations is None:\n",
    "        activations = [\"tanh\", \"linear\"]\n",
    "    elif callable(activations):\n",
    "        activations = activations(ln)\n",
    "\n",
    "    if regularizer_args is None:\n",
    "        regularizer_args = dict()\n",
    "\n",
    "    number_of_layers = len(nodes)\n",
    "    last_layer_nodes = nodes[-1]  # (== len(flav_info))\n",
    "\n",
    "    # First prepare the input for the PDF model and any scaling if needed\n",
    "    placeholder_input = Input(shape=(None, 1), batch_size=1)\n",
    "\n",
    "    subtract_one = False\n",
    "    process_input = Lambda(lambda x: x)\n",
    "    input_x_eq_1 = [1.0]\n",
    "    if scaler:\n",
    "        # change the input domain [0,1] -> [-1,1]\n",
    "        process_input = Lambda(lambda x: 2 * x - 1)\n",
    "        subtract_one = True\n",
    "        input_x_eq_1 = scaler([1.0])[0]\n",
    "        placeholder_input = Input(shape=(None, 2), batch_size=1)\n",
    "    elif inp == 2:\n",
    "        # create a x --> (x, logx) layer to preppend to everything\n",
    "        process_input = Lambda(lambda x: op.concatenate([x, op.op_log(x)], axis=-1))\n",
    "\n",
    "    model_input = [placeholder_input]\n",
    "    if subtract_one:\n",
    "        layer_x_eq_1 = op.numpy_to_input(np.array(input_x_eq_1).reshape(1, 1))\n",
    "        model_input.append(layer_x_eq_1)\n",
    "\n",
    "    # Evolution layer\n",
    "    layer_evln = FkRotation(input_shape=(last_layer_nodes,), output_dim=out)\n",
    "    # Basis rotation\n",
    "    basis_rotation = FlavourToEvolution(flav_info=flav_info, fitbasis=fitbasis)\n",
    "    # Normalization and sum rules\n",
    "    if impose_sumrule:\n",
    "        sumrule_layer, integrator_input = msr_impose(mode=impose_sumrule, scaler=scaler)\n",
    "        model_input.append(integrator_input)\n",
    "    else:\n",
    "        sumrule_layer = lambda x: x\n",
    "\n",
    "    pdf_models = []\n",
    "    for i, layer_seed in enumerate(seed):\n",
    "        if layer_type == \"dense\":\n",
    "            reg = regularizer_selector(regularizer, **regularizer_args)\n",
    "            list_of_pdf_layers = generate_dense_network(\n",
    "                inp,\n",
    "                nodes,\n",
    "                activations,\n",
    "                initializer_name,\n",
    "                As_number=As_number,\n",
    "                seed=layer_seed,\n",
    "                dropout_rate=dropout,\n",
    "                regularizer=reg,\n",
    "            )\n",
    "        elif layer_type == \"dense_per_flavour\":\n",
    "            list_of_pdf_layers = generate_dense_per_flavour_network(\n",
    "                inp,\n",
    "                nodes,\n",
    "                activations,\n",
    "                initializer_name,\n",
    "                seed=layer_seed,\n",
    "                basis_size=last_layer_nodes,\n",
    "            )\n",
    "\n",
    "        def dense_me(x):\n",
    "            \"\"\"Takes an input tensor `x` and applies all layers\n",
    "            from the `list_of_pdf_layers` in order\"\"\"\n",
    "            processed_x = process_input(x)\n",
    "            curr_fun = list_of_pdf_layers[0](processed_x)\n",
    "\n",
    "            for dense_layer in list_of_pdf_layers[1:]:\n",
    "                curr_fun = dense_layer(curr_fun)\n",
    "            return curr_fun\n",
    "\n",
    "        preproseed = layer_seed + number_of_layers\n",
    "        layer_preproc = Preprocessing(\n",
    "            flav_info=flav_info,\n",
    "            input_shape=(1,),\n",
    "            name=f\"pdf_prepro_{i}\",\n",
    "            seed=preproseed,\n",
    "            large_x=not subtract_one,\n",
    "        )\n",
    "\n",
    "        # Apply preprocessing and basis\n",
    "        def layer_fitbasis(x):\n",
    "            x_scaled = op.op_gather_keep_dims(x, 0, axis=-1)\n",
    "            x_original = op.op_gather_keep_dims(x, -1, axis=-1)\n",
    "\n",
    "            nn_output = dense_me(x_scaled)\n",
    "            if subtract_one:\n",
    "                nn_at_one = dense_me(layer_x_eq_1)\n",
    "                nn_output = op.op_subtract([nn_output, nn_at_one])\n",
    "\n",
    "            # Ignore Preprocessing for the Time Being. Still thinking of\n",
    "            # The best way to take preprocessing into account, whether a\n",
    "            # same flavour for different A's should be the same.\n",
    "            # ret = op.op_multiply([nn_output, layer_preproc(x_original)])\n",
    "            ret = nn_output\n",
    "            if basis_rotation.is_identity():\n",
    "                return ret\n",
    "            return basis_rotation(ret)\n",
    "\n",
    "        # Rotation layer, changes from the 8-basis to the 14-basis\n",
    "        def layer_pdf(x):\n",
    "            return layer_evln(layer_fitbasis(x))\n",
    "\n",
    "        # Final PDF (apply normalization)\n",
    "        final_pdf = sumrule_layer(layer_pdf)\n",
    "\n",
    "        # Create the model\n",
    "        pdf_model = MetaModel(\n",
    "            model_input, final_pdf(placeholder_input), name=f\"PDF_{i}\", scaler=scaler\n",
    "        )\n",
    "        pdf_models.append(pdf_model)\n",
    "        pdf_model.summary()\n",
    "    return pdf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5a9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "flav_info = [\n",
    "    {'fl': 'sng', 'trainable': False, 'smallx': [1.094, 1.118], 'largex': [1.46, 3.003]}, \n",
    "    {'fl': 'g', 'trainable': False, 'smallx': [0.8189, 1.044], 'largex': [2.791, 5.697]}, \n",
    "    {'fl': 'v', 'trainable': False, 'smallx': [0.457, 0.7326], 'largex': [1.56, 3.431]}, \n",
    "    {'fl': 'v3', 'trainable': False, 'smallx': [0.1462, 0.4061], 'largex': [1.745, 3.452]}, \n",
    "    {'fl': 'v8', 'trainable': False, 'smallx': [0.5401, 0.7665], 'largex': [1.539, 3.393]}, \n",
    "    {'fl': 't3', 'trainable': False, 'smallx': [-0.4401, 0.9163], 'largex': [1.773, 3.333]}, \n",
    "    {'fl': 't8', 'trainable': False, 'smallx': [0.5852, 0.8537], 'largex': [1.533, 3.436]}, \n",
    "    {'fl': 't15', 'trainable': False, 'smallx':[1.082, 1.142], 'largex': [1.461, 3.1]}\n",
    "]\n",
    "fitbasis = 'EVOL'\n",
    "seed = [1872583848]\n",
    "As_number = 4\n",
    "nodes_in = 2\n",
    "nodes = [3, 6, 8]\n",
    "activations = ['tanh', 'tanh', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b8cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PDF_0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(1, None, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "integration_grid (InputLayer)   [(1, 2000, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (1, None, 1)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (1, 2000, 1)         0           integration_grid[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               multiple             0           lambda_4[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   multiple             9           lambda_1[0][0]                   \n",
      "                                                                 lambda_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 multiple             24          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 multiple             224         dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flavour_to_evolution (FlavourTo multiple             0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "evolution (FkRotation)          multiple             0           flavour_to_evolution[0][0]       \n",
      "                                                                 flavour_to_evolution[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (1, 2000, 1)         0           integration_grid[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "x_divide (xDivide)              (1, 2000, 14)        0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(1, 2000, 14), (1,  0           evolution[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (1, 2000, 14)        0           x_divide[0][0]                   \n",
      "                                                                 tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (1, 2000, 14)        0           x_divide[0][0]                   \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (1, 2000, 14)        0           x_divide[0][0]                   \n",
      "                                                                 tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (1, 2000, 14)        0           x_divide[0][0]                   \n",
      "                                                                 tf_op_layer_split[0][3]          \n",
      "__________________________________________________________________________________________________\n",
      "x_integrator (xIntegrator)      (1, 14)              0           multiply[0][0]                   \n",
      "                                                                 multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "normalizer (MSR_Normalization)  (14,)                0           x_integrator[0][0]               \n",
      "                                                                 x_integrator[1][0]               \n",
      "                                                                 x_integrator[2][0]               \n",
      "                                                                 x_integrator[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ConcatV2 (TensorFlo [(56,)]              0           normalizer[0][0]                 \n",
      "                                                                 normalizer[1][0]                 \n",
      "                                                                 normalizer[2][0]                 \n",
      "                                                                 normalizer[3][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(1, None, 56)]      0           evolution[1][0]                  \n",
      "                                                                 tf_op_layer_ConcatV2[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 257\n",
      "Trainable params: 257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pdf_gen = pdfNN_layer_generator(\n",
    "    nodes=nodes, \n",
    "    activations=activations, \n",
    "    As_number=As_number,\n",
    "    flav_info=flav_info,\n",
    "    fitbasis=fitbasis,\n",
    "    seed=seed,\n",
    "    impose_sumrule=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bdc6a",
   "metadata": {},
   "source": [
    "### ⚠ Implementation below not working yet ⚠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836de23",
   "metadata": {},
   "source": [
    "### 4.4 Construct the Observable\n",
    "\n",
    "Now, we can implement the part that computes the Observable ($\\mathcal{O}^{\\rm th}$) expressed in the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_unique(list_of_arrays):\n",
    "    \"\"\" Check whether the list of arrays more than one different arrays \"\"\"\n",
    "    the_first = list_of_arrays[0]\n",
    "    for i in list_of_arrays[1:]:\n",
    "        if not np.array_equal(the_first, i):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class Observable(MetaLayer, ABC):\n",
    "    \"\"\"\n",
    "        This class is the parent of the DIS and DY convolutions.\n",
    "        All backend-dependent code necessary for the convolutions\n",
    "                                    is (must be) concentrated here\n",
    "\n",
    "        The methods gen_mask and call must be overriden by the observables\n",
    "        where\n",
    "            - gen_mask: it is called by the initializer and generates the mask between\n",
    "                        fktables and pdfs\n",
    "            - call: this is what does the actual operation\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            fktable_dicts: list\n",
    "                list of fktable_dicts which define basis and xgrid for the fktables in the list\n",
    "            fktable_arr: list\n",
    "                list of fktables for this observable\n",
    "            operation_name: str\n",
    "                string defining the name of the operation to be applied to the fktables\n",
    "            nfl: int\n",
    "                number of flavours in the pdf (default:14)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fktable_dicts, fktable_arr, operation_name, nfl=14, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Descriptions:\n",
    "        -------------\n",
    "        fktable_dicts: \n",
    "            list of dictionaries where each element contains some specifictions\n",
    "            about a given dataset. It has the following keys:\n",
    "            - ndata{N_dpts}:  nb of datapoints\n",
    "            - nbasis:\n",
    "            - nonzero: nb of nonzero PDFs\n",
    "            - basis: gives the indices of the non-zero PDFs\n",
    "            - nx{N_x}: size of the z-grid\n",
    "            - xgrid: array of x-points\n",
    "            - fktable: Tensor of shape (N_dpts, nonzero_fl, N_x)\n",
    "\n",
    "        fktable_arr:\n",
    "        \"\"\"\n",
    "        super(MetaLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.nfl = nfl\n",
    "\n",
    "        basis = []\n",
    "        xgrids = []\n",
    "        self.fktables = []\n",
    "        for fktable, fk in zip(fktable_dicts, fktable_arr):\n",
    "            xgrids.append(fktable[\"xgrid\"])\n",
    "            basis.append(fktable[\"basis\"])\n",
    "            self.fktables.append(op.numpy_to_tensor(fk))\n",
    "\n",
    "        # check how many xgrids this dataset needs\n",
    "        if _is_unique(xgrids):\n",
    "            self.splitting = None\n",
    "        else:\n",
    "            self.splitting = [i.shape[1] for i in xgrids]\n",
    "\n",
    "        # check how many basis this dataset needs\n",
    "        if _is_unique(basis) and _is_unique(xgrids):\n",
    "            self.all_masks = [self.gen_mask(basis[0])]\n",
    "            self.many_masks = False\n",
    "        else:\n",
    "            self.many_masks = True\n",
    "            self.all_masks = [self.gen_mask(i) for i in basis]\n",
    "\n",
    "        self.operation = op.c_to_py_fun(operation_name)\n",
    "        self.output_dim = self.fktables[0].shape[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (self.output_dim, None)\n",
    "\n",
    "    # Overridables\n",
    "    @abstractmethod\n",
    "    def gen_mask(self, basis):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIS(Observable):\n",
    "\n",
    "    def gen_mask(self, basis):\n",
    "\n",
    "        if basis is None:\n",
    "            self.basis = np.ones(self.nfl, dtype=bool)\n",
    "        else:\n",
    "            basis_mask = np.zeros(self.nfl, dtype=bool)\n",
    "            for i in basis:\n",
    "                basis_mask[i] = True\n",
    "        return op.numpy_to_tensor(basis_mask, dtype=bool)\n",
    "\n",
    "    def call(self, pdf):\n",
    "\n",
    "        # DIS never needs splitting\n",
    "        if self.splitting is not None:\n",
    "            raise ValueError(\"DIS layer call with a dataset that needs more than one xgrid?\")\n",
    "\n",
    "        results = []\n",
    "        # Separate the two possible paths this layer can take\n",
    "        if self.many_masks:\n",
    "            for mask, fktable in zip(self.all_masks, self.fktables):\n",
    "                pdf_masked = op.boolean_mask(pdf, mask, axis=2)\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        else:\n",
    "            pdf_masked = op.boolean_mask(pdf, self.all_masks[0], axis=2)\n",
    "            for fktable in self.fktables:\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        return self.operation(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n3fit.layers.losses import LossPositivity\n",
    "from n3fit.layers.losses import LossInvcovmat\n",
    "from n3fit.layers.losses import LossIntegrability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ObservableWrapper:\n",
    "\n",
    "    name: str\n",
    "    observables: list\n",
    "    dataset_xsizes: list\n",
    "    invcovmat: np.array = None\n",
    "    covmat: np.array = None\n",
    "    multiplier: float = 1.0\n",
    "    integrability: bool = False\n",
    "    positivity: bool = False\n",
    "    data: np.array = None\n",
    "    rotation: ObsRotation = None  # only used for diagonal covmat\n",
    "\n",
    "    def _generate_loss(self, mask=None):\n",
    "        \n",
    "        if self.invcovmat is not None:\n",
    "            loss = losses.LossInvcovmat(\n",
    "                self.invcovmat, self.data, mask, covmat=self.covmat, name=self.name\n",
    "            )\n",
    "        elif self.positivity:\n",
    "            loss = losses.LossPositivity(name=self.name, c=self.multiplier)\n",
    "        elif self.integrability:\n",
    "            loss = losses.LossIntegrability(name=self.name, c=self.multiplier)\n",
    "        return loss\n",
    "\n",
    "    def _generate_experimental_layer(self, pdf):\n",
    "        \n",
    "        if len(self.dataset_xsizes) > 1:\n",
    "            splitting_layer = op.as_layer(\n",
    "                op.split,\n",
    "                op_args=[self.dataset_xsizes],\n",
    "                op_kwargs={\"axis\": 1},\n",
    "                name=f\"{self.name}_split\",\n",
    "            )\n",
    "            split_pdf = splitting_layer(pdf)\n",
    "        else:\n",
    "            split_pdf = [pdf]\n",
    "        output_layers = [obs(p_pdf) for p_pdf, obs in zip(split_pdf, self.observables)]\n",
    "        ret = op.concatenate(output_layers, axis=2)\n",
    "        if self.rotation is not None:\n",
    "            ret = self.rotation(ret)\n",
    "        return ret\n",
    "\n",
    "    def __call__(self, pdf_layer, mask=None):\n",
    "        loss_f = self._generate_loss(mask)\n",
    "        experiment_prediction = self._generate_experimental_layer(pdf_layer)\n",
    "        return loss_f(experiment_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf37310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observable_generator(\n",
    "    spec_dict, positivity_initial=1.0, integrability=False\n",
    "):\n",
    "    \n",
    "    spec_name = spec_dict[\"name\"]\n",
    "    dataset_xsizes = []\n",
    "    model_obs_tr = []\n",
    "    model_obs_vl = []\n",
    "    model_obs_ex = []\n",
    "    model_inputs = []\n",
    "\n",
    "    for dataset_dict in spec_dict[\"datasets\"]:\n",
    "        dataset_name = dataset_dict[\"name\"]\n",
    "\n",
    "        if dataset_dict[\"hadronic\"]:\n",
    "            Obs_Layer = DY\n",
    "        else:\n",
    "            Obs_Layer = DIS\n",
    "\n",
    "        operation_name = dataset_dict[\"operation\"]\n",
    "\n",
    "        if spec_dict[\"positivity\"]:\n",
    "            obs_layer_tr = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"tr_fktables\"],\n",
    "                operation_name,\n",
    "                name=f\"dat_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_ex = obs_layer_vl = None\n",
    "        elif spec_dict.get(\"data_transformation_tr\") is not None:\n",
    "            obs_layer_ex = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"ex_fktables\"],\n",
    "                operation_name,\n",
    "                name=f\"exp_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_tr = obs_layer_vl = obs_layer_ex\n",
    "        else:\n",
    "            obs_layer_tr = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"tr_fktables\"],\n",
    "                operation_name,\n",
    "                name=f\"dat_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_ex = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"ex_fktables\"],\n",
    "                operation_name,\n",
    "                name=f\"exp_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_vl = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"vl_fktables\"],\n",
    "                operation_name,\n",
    "                name=f\"val_{dataset_name}\",\n",
    "            )\n",
    "\n",
    "        if obs_layer_tr.splitting is None:\n",
    "            xgrid = dataset_dict[\"fktables\"][0][\"xgrid\"]\n",
    "            model_inputs.append(xgrid)\n",
    "            dataset_xsizes.append(xgrid.shape[1])\n",
    "        else:\n",
    "            xgrids = [i[\"xgrid\"] for i in dataset_dict[\"fktables\"]]\n",
    "            model_inputs += xgrids\n",
    "            dataset_xsizes.append(sum([i.shape[1] for i in xgrids]))\n",
    "\n",
    "        model_obs_tr.append(obs_layer_tr)\n",
    "        model_obs_vl.append(obs_layer_vl)\n",
    "        model_obs_ex.append(obs_layer_ex)\n",
    "\n",
    "    full_nx = sum(dataset_xsizes)\n",
    "    if spec_dict[\"positivity\"]:\n",
    "        out_positivity = ObservableWrapper(\n",
    "            spec_name,\n",
    "            model_obs_tr,\n",
    "            dataset_xsizes,\n",
    "            multiplier=positivity_initial,\n",
    "            positivity=not integrability,\n",
    "            integrability=integrability,\n",
    "        )\n",
    "\n",
    "        layer_info = {\n",
    "            \"inputs\": model_inputs,\n",
    "            \"output_tr\": out_positivity,\n",
    "            \"experiment_xsize\": full_nx,\n",
    "        }\n",
    "        return layer_info\n",
    "\n",
    "    if spec_dict.get(\"data_transformation_tr\") is not None:\n",
    "        obsrot_tr = ObsRotation(spec_dict.get(\"data_transformation_tr\"))\n",
    "        obsrot_vl = ObsRotation(spec_dict.get(\"data_transformation_vl\"))\n",
    "    else:\n",
    "        obsrot_tr = None\n",
    "        obsrot_vl = None\n",
    "\n",
    "    out_tr = ObservableWrapper(\n",
    "        spec_name,\n",
    "        model_obs_tr,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat\"],\n",
    "        data=spec_dict[\"expdata\"],\n",
    "        rotation=obsrot_tr,\n",
    "    )\n",
    "    out_vl = ObservableWrapper(\n",
    "        f\"{spec_name}_val\",\n",
    "        model_obs_vl,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_vl\"],\n",
    "        data=spec_dict[\"expdata_vl\"],\n",
    "        rotation=obsrot_vl,\n",
    "    )\n",
    "    out_exp = ObservableWrapper(\n",
    "        f\"{spec_name}_exp\",\n",
    "        model_obs_ex,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_true\"],\n",
    "        covmat=spec_dict[\"covmat\"],\n",
    "        data=spec_dict[\"expdata_true\"],\n",
    "        rotation=None,\n",
    "    )\n",
    "\n",
    "    layer_info = {\n",
    "        \"inputs\": model_inputs,\n",
    "        \"output\": out_exp,\n",
    "        \"output_tr\": out_tr,\n",
    "        \"output_vl\": out_vl,\n",
    "        \"experiment_xsize\": full_nx,\n",
    "    }\n",
    "    return layer_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954bfbc",
   "metadata": {},
   "source": [
    "What remains to do now is to combined everything and construct a class to perform a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb872f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "def _pdf_injection(pdf_layers, observables, masks):\n",
    "    \"\"\"Takes as input a list of PDF layers and if needed applies masks.\"\"\"\n",
    "    return [f(x, mask=m) for f, x, m in zip_longest(observables, pdf_layers, masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_POSITIVITY_EACH = 100\n",
    "PUSH_INTEGRABILITY_EACH = 100\n",
    "CHI2_THRESHOLD = 10.0\n",
    "\n",
    "def _LM_initial_and_multiplier(input_initial, input_multiplier, max_lambda, steps):\n",
    "    initial, multiplier = input_initial, input_multiplier\n",
    "    if multiplier is None:\n",
    "        if initial is None: initial = 1.0\n",
    "        multiplier = pow(max_lambda / initial, 1 / max(steps, 1))\n",
    "    elif initial is None:\n",
    "        initial = max_lambda / pow(multiplier, steps)\n",
    "    return initial, multiplier\n",
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exp_info,\n",
    "        pos_info,\n",
    "        integ_info,\n",
    "        flavinfo,\n",
    "        fitbasis,\n",
    "        nnseeds,\n",
    "        pass_status=\"ok\",\n",
    "        failed_status=\"fail\",\n",
    "        debug=False,\n",
    "        kfold_parameters=None,\n",
    "        max_cores=None,\n",
    "        model_file=None,\n",
    "        sum_rules=None,\n",
    "        parallel_models=1,\n",
    "    ):\n",
    "        \n",
    "        # Save all input information\n",
    "        self.exp_info = exp_info\n",
    "        self.pos_info = pos_info\n",
    "        self.integ_info = integ_info\n",
    "        if self.integ_info is not None:\n",
    "            self.all_info = exp_info + pos_info + integ_info\n",
    "        else:\n",
    "            self.all_info = exp_info + pos_info\n",
    "        self.flavinfo = flavinfo\n",
    "        self.fitbasis = fitbasis\n",
    "        self._nn_seeds = nnseeds\n",
    "        self.pass_status = pass_status\n",
    "        self.failed_status = failed_status\n",
    "        self.debug = debug\n",
    "        self.all_datasets = []\n",
    "        self._scaler = None\n",
    "        self._parallel_models = parallel_models\n",
    "\n",
    "        # Initialise internal variables which define behaviour\n",
    "        if debug:\n",
    "            self.max_cores = 1\n",
    "        else:\n",
    "            self.max_cores = max_cores\n",
    "        self.model_file = model_file\n",
    "        self.print_summary = True\n",
    "        self.mode_hyperopt = False\n",
    "        self.impose_sumrule = sum_rules\n",
    "        self._hyperkeys = None\n",
    "        if kfold_parameters is None:\n",
    "            self.kpartitions = [None]\n",
    "            self.hyper_threshold = None\n",
    "        else:\n",
    "            self.kpartitions = kfold_parameters[\"partitions\"]\n",
    "            self.hyper_threshold = kfold_parameters.get(\"threshold\", HYPER_THRESHOLD)\n",
    "            # if there are penalties enabled, set them up\n",
    "            penalties = kfold_parameters.get(\"penalties\", [])\n",
    "            self.hyper_penalties = []\n",
    "            for penalty in penalties:\n",
    "                pen_fun = getattr(n3fit.hyper_optimization.penalties, penalty)\n",
    "                self.hyper_penalties.append(pen_fun)\n",
    "                log.info(\"Adding penalty: %s\", penalty)\n",
    "            # Check what is the hyperoptimization target function\n",
    "            hyper_loss = kfold_parameters.get(\"target\", None)\n",
    "            if hyper_loss is None:\n",
    "                hyper_loss = \"average\"\n",
    "                log.warning(\"No minimization target selected, defaulting to '%s'\", hyper_loss)\n",
    "            log.info(\"Using '%s' as the target for hyperoptimization\", hyper_loss)\n",
    "            self._hyper_loss = getattr(n3fit.hyper_optimization.rewards, hyper_loss)\n",
    "\n",
    "        # Initialize the dictionaries which contain all fitting information\n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        self.training = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"posdatasets\": [],\n",
    "            \"posmultipliers\": [],\n",
    "            \"posinitials\": [],\n",
    "            \"integdatasets\": [],\n",
    "            \"integmultipliers\": [],\n",
    "            \"integinitials\": [],\n",
    "            \"folds\": [],\n",
    "        }\n",
    "        self.validation = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "            \"posdatasets\": [],\n",
    "        }\n",
    "        self.experimental = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "        }\n",
    "\n",
    "        self._fill_the_dictionaries()\n",
    "\n",
    "        if self.validation[\"ndata\"] == 0:\n",
    "            # If there is no validation, the validation chi2 = training chi2\n",
    "            self.no_validation = True\n",
    "            self.validation[\"expdata\"] = self.training[\"expdata\"]\n",
    "        else:\n",
    "            # Consider the validation only if there is validation (of course)\n",
    "            self.no_validation = False\n",
    "\n",
    "        self.callbacks = []\n",
    "        if debug:\n",
    "            self.callbacks.append(callbacks.TimerCallback())\n",
    "\n",
    "    def set_hyperopt(self, hyperopt_on, keys=None, status_ok=\"ok\"):\n",
    "        \"\"\"Set hyperopt options on and off (mostly suppresses some printing)\"\"\"\n",
    "        self.pass_status = status_ok\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "        self._hyperkeys = keys\n",
    "        if hyperopt_on:\n",
    "            self.print_summary = False\n",
    "            self.mode_hyperopt = True\n",
    "        else:\n",
    "            self.print_summary = True\n",
    "            self.mode_hyperopt = False\n",
    "\n",
    "            \n",
    "    def _fill_the_dictionaries(self):\n",
    "        \"\"\"\n",
    "        This function fills the following dictionaries\n",
    "            -``training``: data for the fit\n",
    "            -``validation``: data which for the stopping\n",
    "            -``experimental``: 'true' data, only used for reporting purposes\n",
    "        with fixed information.\n",
    "\n",
    "        Fixed information: information which will not change between different runs of the code.\n",
    "        This information does not depend on the parameters of the fit at any stage\n",
    "        and so it will remain unchanged between different runs of the hyperoptimizer.\n",
    "\n",
    "        The aforementioned information corresponds to:\n",
    "            - ``expdata``: experimental data\n",
    "            - ``name``: names of the experiment\n",
    "            - ``ndata``: number of experimental points\n",
    "        \"\"\"\n",
    "        for exp_dict in self.exp_info:\n",
    "            self.training[\"expdata\"].append(exp_dict[\"expdata\"])\n",
    "            self.validation[\"expdata\"].append(exp_dict[\"expdata_vl\"])\n",
    "            self.experimental[\"expdata\"].append(exp_dict[\"expdata_true\"])\n",
    "\n",
    "            self.training[\"folds\"].append(exp_dict[\"folds\"][\"training\"])\n",
    "            self.validation[\"folds\"].append(exp_dict[\"folds\"][\"validation\"])\n",
    "            self.experimental[\"folds\"].append(exp_dict[\"folds\"][\"experimental\"])\n",
    "\n",
    "            nd_tr = exp_dict[\"ndata\"]\n",
    "            nd_vl = exp_dict[\"ndata_vl\"]\n",
    "\n",
    "            self.training[\"ndata\"] += nd_tr\n",
    "            self.validation[\"ndata\"] += nd_vl\n",
    "            self.experimental[\"ndata\"] += nd_tr + nd_vl\n",
    "\n",
    "            for dataset in exp_dict[\"datasets\"]:\n",
    "                self.all_datasets.append(dataset[\"name\"])\n",
    "        self.all_datasets = set(self.all_datasets)\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            self.training[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.training[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "            self.validation[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.validation[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                self.training[\"expdata\"].append(integ_dict[\"expdata\"])\n",
    "                self.training[\"integdatasets\"].append(integ_dict[\"name\"])\n",
    "\n",
    "    def _model_generation(self, pdf_models, partition, partition_idx):\n",
    "       \n",
    "        log.info(\"Generating the Model\")\n",
    "        input_arr = np.concatenate(self.input_list, axis=1).T\n",
    "        if self._scaler:\n",
    "            input_arr = self._scaler(input_arr)\n",
    "        input_layer = op.numpy_to_input(input_arr)\n",
    "\n",
    "        all_replicas_pdf = []\n",
    "        for pdf_model in pdf_models:\n",
    "            full_model_input_dict, full_pdf = pdf_model.apply_as_layer([input_layer])\n",
    "\n",
    "            all_replicas_pdf.append(full_pdf)\n",
    "\n",
    "        full_pdf_per_replica = op.stack(all_replicas_pdf, axis=-1)\n",
    "\n",
    "        sp_ar = [self.input_sizes]\n",
    "        sp_kw = {\"axis\": 1}\n",
    "        splitting_layer = op.as_layer(op.split, op_args=sp_ar, op_kwargs=sp_kw, name=\"pdf_split\")\n",
    "        splitted_pdf = splitting_layer(full_pdf_per_replica)\n",
    "\n",
    "        training_mask = validation_mask = experimental_mask = [None]\n",
    "        if partition and partition[\"datasets\"]:\n",
    "            if partition.get(\"overfit\", False):\n",
    "                training_mask = [i[partition_idx] for i in self.training[\"folds\"]]\n",
    "                validation_mask = [i[partition_idx] for i in self.validation[\"folds\"]]\n",
    "            experimental_mask = [i[partition_idx] for i in self.experimental[\"folds\"]]\n",
    "        output_tr = _pdf_injection(splitted_pdf, self.training[\"output\"], training_mask)\n",
    "        training = MetaModel(full_model_input_dict, output_tr)\n",
    "\n",
    "        val_pdfs = []\n",
    "        exp_pdfs = []\n",
    "        for partial_pdf, obs in zip(splitted_pdf, self.training[\"output\"]):\n",
    "            if not obs.positivity and not obs.integrability:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "                exp_pdfs.append(partial_pdf)\n",
    "            elif not obs.integrability and obs.positivity:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "\n",
    "        output_vl = _pdf_injection(val_pdfs, self.validation[\"output\"], validation_mask)\n",
    "        validation = MetaModel(full_model_input_dict, output_vl)\n",
    "\n",
    "        output_ex = _pdf_injection(exp_pdfs, self.experimental[\"output\"], experimental_mask)\n",
    "        experimental = MetaModel(full_model_input_dict, output_ex)\n",
    "\n",
    "        if self.print_summary:\n",
    "            training.summary()\n",
    "\n",
    "        models = {\n",
    "            \"training\": training,\n",
    "            \"validation\": validation,\n",
    "            \"experimental\": experimental,\n",
    "        }\n",
    "\n",
    "        return models\n",
    "\n",
    "    def _reset_observables(self):\n",
    "        \n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        for key in [\"output\", \"posmultipliers\", \"integmultipliers\"]:\n",
    "            self.training[key] = []\n",
    "            self.validation[key] = []\n",
    "            self.experimental[key] = []\n",
    "\n",
    "    def _generate_observables(\n",
    "        self,\n",
    "        all_pos_multiplier,\n",
    "        all_pos_initial,\n",
    "        all_integ_multiplier,\n",
    "        all_integ_initial,\n",
    "        epochs,\n",
    "        interpolation_points,\n",
    "    ):\n",
    "        \n",
    "        self._reset_observables()\n",
    "        log.info(\"Generating layers\")\n",
    "        for exp_dict in self.exp_info:\n",
    "            if not self.mode_hyperopt:\n",
    "                log.info(\"Generating layers for experiment %s\", exp_dict[\"name\"])\n",
    "\n",
    "            exp_layer = observable_generator(exp_dict)\n",
    "\n",
    "            self.input_list += exp_layer[\"inputs\"]\n",
    "            self.input_sizes.append(exp_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(exp_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(exp_layer[\"output_vl\"])\n",
    "            self.experimental[\"output\"].append(exp_layer[\"output\"])\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            if not self.mode_hyperopt:\n",
    "                log.info(\"Generating positivity penalty for %s\", pos_dict[\"name\"])\n",
    "\n",
    "            positivity_steps = int(epochs / PUSH_POSITIVITY_EACH)\n",
    "            max_lambda = pos_dict[\"lambda\"]\n",
    "\n",
    "            pos_initial, pos_multiplier = _LM_initial_and_multiplier(\n",
    "                all_pos_initial, all_pos_multiplier, max_lambda, positivity_steps\n",
    "            )\n",
    "\n",
    "            pos_layer = observable_generator(pos_dict, positivity_initial=pos_initial)\n",
    "            self.input_list += pos_layer[\"inputs\"]\n",
    "            self.input_sizes.append(pos_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "\n",
    "            self.training[\"posmultipliers\"].append(pos_multiplier)\n",
    "            self.training[\"posinitials\"].append(pos_initial)\n",
    "\n",
    "        # Finally generate the integrability penalty\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                if not self.mode_hyperopt:\n",
    "                    log.info(\"Generating integrability penalty for %s\", integ_dict[\"name\"])\n",
    "\n",
    "                integrability_steps = int(epochs / PUSH_INTEGRABILITY_EACH)\n",
    "                max_lambda = integ_dict[\"lambda\"]\n",
    "\n",
    "                integ_initial, integ_multiplier = _LM_initial_and_multiplier(\n",
    "                    all_integ_initial, all_integ_multiplier, max_lambda, integrability_steps\n",
    "                )\n",
    "\n",
    "                integ_layer = observable_generator(\n",
    "                    integ_dict, positivity_initial=integ_initial, integrability=True\n",
    "                )\n",
    "                self.input_list += integ_layer[\"inputs\"]\n",
    "                self.input_sizes.append(integ_layer[\"experiment_xsize\"])\n",
    "                self.training[\"output\"].append(integ_layer[\"output_tr\"])\n",
    "                self.training[\"integmultipliers\"].append(integ_multiplier)\n",
    "                self.training[\"integinitials\"].append(integ_initial)\n",
    "\n",
    "        if interpolation_points:\n",
    "            input_arr = np.concatenate(self.input_list, axis=1)\n",
    "            input_arr = np.sort(input_arr)\n",
    "            input_arr_size = input_arr.size\n",
    "\n",
    "            force_set_smallest = input_arr.min() > 1e-9\n",
    "            if force_set_smallest:\n",
    "                new_xgrid = np.linspace(\n",
    "                    start=1/input_arr_size, stop=1.0, endpoint=False, num=input_arr_size\n",
    "                )\n",
    "            else:\n",
    "                new_xgrid = np.linspace(start=0, stop=1.0, endpoint=False, num=input_arr_size)\n",
    "\n",
    "            unique, counts = np.unique(input_arr, return_counts=True)\n",
    "            map_to_complete = []\n",
    "            for cumsum_ in np.cumsum(counts):\n",
    "                map_to_complete.append(new_xgrid[cumsum_ - counts[0]])\n",
    "            map_to_complete = np.array(map_to_complete)\n",
    "            map_from_complete = unique\n",
    "\n",
    "            if force_set_smallest:\n",
    "                map_from_complete = np.insert(map_from_complete, 0, 1e-9)\n",
    "                map_to_complete = np.insert(map_to_complete, 0, 0.0)\n",
    "\n",
    "            onein = map_from_complete.size / (int(interpolation_points) - 1)\n",
    "            selected_points = [round(i * onein - 1) for i in range(1, int(interpolation_points))]\n",
    "            if selected_points[0] != 0:\n",
    "                selected_points = [0] + selected_points\n",
    "            map_from = map_from_complete[selected_points]\n",
    "            map_from = np.log(map_from)\n",
    "            map_to = map_to_complete[selected_points]\n",
    "\n",
    "            try:\n",
    "                scaler = PchipInterpolator(map_from, map_to)\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    \"interpolation_points is larger than the number of unique \"\n",
    "                                    \"input x-values\"\n",
    "                )\n",
    "            self._scaler = lambda x: np.concatenate([scaler(np.log(x)), x], axis=-1)\n",
    "\n",
    "    def _generate_pdf(\n",
    "        self,\n",
    "        nodes_per_layer,\n",
    "        activation_per_layer,\n",
    "        initializer,\n",
    "        layer_type,\n",
    "        dropout,\n",
    "        regularizer,\n",
    "        regularizer_args,\n",
    "        seed,\n",
    "    ):\n",
    "        log.info(\"Generating PDF models\")\n",
    "\n",
    "        pdf_models = pdfNN_layer_generator(\n",
    "            nodes=nodes_per_layer,\n",
    "            activations=activation_per_layer,\n",
    "            layer_type=layer_type,\n",
    "            flav_info=self.flavinfo,\n",
    "            fitbasis=self.fitbasis,\n",
    "            seed=seed,\n",
    "            initializer_name=initializer,\n",
    "            dropout=dropout,\n",
    "            regularizer=regularizer,\n",
    "            regularizer_args=regularizer_args,\n",
    "            impose_sumrule=self.impose_sumrule,\n",
    "            scaler=self._scaler,\n",
    "            parallel_models=self._parallel_models,\n",
    "        )\n",
    "        return pdf_models\n",
    "\n",
    "    def _prepare_reporting(self, partition):\n",
    "\n",
    "        reported_keys = [\"name\", \"count_chi2\", \"positivity\", \"integrability\", \"ndata\", \"ndata_vl\"]\n",
    "        reporting_list = []\n",
    "        for exp_dict in self.all_info:\n",
    "            reporting_dict = {k: exp_dict.get(k) for k in reported_keys}\n",
    "            if partition:\n",
    "                for dataset in exp_dict[\"datasets\"]:\n",
    "                    if dataset in partition[\"datasets\"]:\n",
    "                        ndata = dataset[\"ndata\"]\n",
    "                        frac = dataset[\"frac\"]\n",
    "                        reporting_dict[\"ndata\"] -= int(ndata * frac)\n",
    "                        reporting_dict[\"ndata_vl\"] = int(ndata * (1 - frac))\n",
    "            reporting_list.append(reporting_dict)\n",
    "        return reporting_list\n",
    "\n",
    "    def _train_and_fit(self, training_model, stopping_object, epochs=100):\n",
    "        \n",
    "        callback_st = callbacks.StoppingCallback(stopping_object)\n",
    "        callback_pos = callbacks.LagrangeCallback(\n",
    "            self.training[\"posdatasets\"],\n",
    "            self.training[\"posmultipliers\"],\n",
    "            update_freq=PUSH_POSITIVITY_EACH,\n",
    "        )\n",
    "        callback_integ = callbacks.LagrangeCallback(\n",
    "            self.training[\"integdatasets\"],\n",
    "            self.training[\"integmultipliers\"],\n",
    "            update_freq=PUSH_INTEGRABILITY_EACH,\n",
    "        )\n",
    "\n",
    "        training_model.perform_fit(\n",
    "            epochs=epochs,\n",
    "            verbose=False,\n",
    "            callbacks=self.callbacks + [callback_st, callback_pos, callback_integ],\n",
    "        )\n",
    "\n",
    "        if any(bool(i) for i in stopping_object.e_best_chi2):\n",
    "            return self.pass_status\n",
    "        return self.failed_status\n",
    "\n",
    "    def _hyperopt_override(self, params):\n",
    "        \n",
    "        hyperparameters = params.get(\"parameters\")\n",
    "        if hyperparameters is not None:\n",
    "            return hyperparameters\n",
    "        for hyperkey in self._hyperkeys:\n",
    "            item = params[hyperkey]\n",
    "            if isinstance(item, dict):\n",
    "                params.update(item)\n",
    "        return params\n",
    "\n",
    "    def enable_tensorboard(self, logdir, weight_freq=0, profiling=False):\n",
    "        \n",
    "        callback_tb = callbacks.gen_tensorboard_callback(\n",
    "            logdir, profiling=profiling, histogram_freq=weight_freq\n",
    "        )\n",
    "        self.callbacks.append(callback_tb)\n",
    "\n",
    "    def evaluate(self, stopping_object):\n",
    "        \n",
    "        if self.training[\"model\"] is None:\n",
    "            raise RuntimeError(\"Modeltrainer.evaluate was called before any training\")\n",
    "        train_chi2 = stopping_object.evaluate_training(self.training[\"model\"])\n",
    "        val_chi2 = stopping_object.vl_chi2\n",
    "        exp_chi2 = self.experimental[\"model\"].compute_losses()[\"loss\"] / self.experimental[\"ndata\"]\n",
    "        return train_chi2, val_chi2, exp_chi2\n",
    "\n",
    "    def hyperparametrizable(self, params):\n",
    "        \n",
    "        clear_backend_state()\n",
    "\n",
    "        if self.mode_hyperopt:\n",
    "            log.info(\"Performing hyperparameter scan\")\n",
    "            for key in self._hyperkeys:\n",
    "                log.info(\" > > Testing %s = %s\", key, params[key])\n",
    "            params = self._hyperopt_override(params)\n",
    "\n",
    "        epochs = int(params[\"epochs\"])\n",
    "        stopping_patience = params[\"stopping_patience\"]\n",
    "        stopping_epochs = int(epochs * stopping_patience)\n",
    "\n",
    "        positivity_dict = params.get(\"positivity\", {})\n",
    "        integrability_dict = params.get(\"integrability\", {})\n",
    "        self._generate_observables(\n",
    "            positivity_dict.get(\"multiplier\"),\n",
    "            positivity_dict.get(\"initial\"),\n",
    "            integrability_dict.get(\"multiplier\"),\n",
    "            integrability_dict.get(\"initial\"),\n",
    "            epochs,\n",
    "            params.get(\"interpolation_points\"),\n",
    "        )\n",
    "        threshold_pos = positivity_dict.get(\"threshold\", 1e-6)\n",
    "        threshold_chi2 = params.get(\"threshold_chi2\", CHI2_THRESHOLD)\n",
    "\n",
    "        # Initialize the chi2 dictionaries\n",
    "        l_valid = []\n",
    "        l_exper = []\n",
    "        l_hyper = []\n",
    "        # And lists to save hyperopt utilities\n",
    "        n3pdfs = []\n",
    "        exp_models = []\n",
    "\n",
    "        ### Training loop\n",
    "        for k, partition in enumerate(self.kpartitions):\n",
    "            seeds = self._nn_seeds\n",
    "            if k > 0:\n",
    "                seeds = [np.random.randint(0, pow(2, 31)) for _ in seeds]\n",
    "\n",
    "            # Generate the pdf model\n",
    "            pdf_models = self._generate_pdf(\n",
    "                params[\"nodes_per_layer\"],\n",
    "                params[\"activation_per_layer\"],\n",
    "                params[\"initializer\"],\n",
    "                params[\"layer_type\"],\n",
    "                params[\"dropout\"],\n",
    "                params.get(\"regularizer\", None),\n",
    "                params.get(\"regularizer_args\", None),\n",
    "                seeds,\n",
    "            )\n",
    "\n",
    "            models = self._model_generation(pdf_models, partition, k)\n",
    "\n",
    "            if self.model_file:\n",
    "                log.info(\"Applying model file %s\", self.model_file)\n",
    "                for pdf_model in pdf_models:\n",
    "                    pdf_model.load_weights(self.model_file)\n",
    "\n",
    "            if k > 0:\n",
    "                pos_and_int = self.training[\"posdatasets\"] + self.training[\"integdatasets\"]\n",
    "                initial_values = self.training[\"posinitials\"] + self.training[\"posinitials\"]\n",
    "                models[\"training\"].reset_layer_weights_to(pos_and_int, initial_values)\n",
    "\n",
    "            # Generate the list containing reporting info necessary for chi2\n",
    "            reporting = self._prepare_reporting(partition)\n",
    "\n",
    "            if self.no_validation:\n",
    "                # Substitute the validation model with the training model\n",
    "                models[\"validation\"] = models[\"training\"]\n",
    "                validation_model = models[\"training\"]\n",
    "            else:\n",
    "                validation_model = models[\"validation\"]\n",
    "\n",
    "            stopping_object = Stopping(\n",
    "                validation_model,\n",
    "                reporting,\n",
    "                pdf_models,\n",
    "                total_epochs=epochs,\n",
    "                stopping_patience=stopping_epochs,\n",
    "                threshold_positivity=threshold_pos,\n",
    "                threshold_chi2=threshold_chi2,\n",
    "            )\n",
    "\n",
    "            for model in models.values():\n",
    "                model.compile(**params[\"optimizer\"])\n",
    "\n",
    "            passed = self._train_and_fit(\n",
    "                models[\"training\"],\n",
    "                stopping_object,\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            if self.mode_hyperopt:\n",
    "                validation_loss = np.mean(stopping_object.vl_chi2)\n",
    "\n",
    "                # Compute experimental loss\n",
    "                exp_loss_raw = np.average(models[\"experimental\"].compute_losses()[\"loss\"])\n",
    "                ndata = np.sum([np.count_nonzero(i[k]) for i in self.experimental[\"folds\"]])\n",
    "                if ndata == 0:\n",
    "                    ndata = self.experimental[\"ndata\"]\n",
    "                experimental_loss = exp_loss_raw / ndata\n",
    "\n",
    "                hyper_loss = experimental_loss\n",
    "                if passed != self.pass_status:\n",
    "                    log.info(\"Hyperparameter combination fail to find a good fit, breaking\")\n",
    "                    break\n",
    "                for penalty in self.hyper_penalties:\n",
    "                    hyper_loss += penalty(pdf_models=pdf_models, stopping_object=stopping_object)\n",
    "                log.info(\"Fold %d finished, loss=%.1f, pass=%s\", k + 1, hyper_loss, passed)\n",
    "\n",
    "                l_hyper.append(hyper_loss)\n",
    "                l_valid.append(validation_loss)\n",
    "                l_exper.append(experimental_loss)\n",
    "                n3pdfs.append(N3PDF(pdf_models, name=f\"fold_{k}\"))\n",
    "                exp_models.append(models[\"experimental\"])\n",
    "\n",
    "                if hyper_loss > self.hyper_threshold:\n",
    "                    log.info(\n",
    "                        \"Loss above threshold (%.1f > %.1f), breaking\",\n",
    "                        hyper_loss,\n",
    "                        self.hyper_threshold,\n",
    "                    )\n",
    "                    pen_mul = len(self.kpartitions) - k\n",
    "                    l_hyper = [i * pen_mul for i in l_hyper]\n",
    "                    break\n",
    "\n",
    "        if self.mode_hyperopt:\n",
    "            dict_out = {\n",
    "                \"status\": passed,\n",
    "                \"loss\": self._hyper_loss(fold_losses=l_hyper, n3pdfs=n3pdfs, experimental_models=exp_models),\n",
    "                \"validation_loss\": np.average(l_valid),\n",
    "                \"experimental_loss\": np.average(l_exper),\n",
    "                \"kfold_meta\": {\n",
    "                    \"validation_losses\": l_valid,\n",
    "                    \"experimental_losses\": l_exper,\n",
    "                    \"hyper_losses\": l_hyper,\n",
    "                },\n",
    "            }\n",
    "            return dict_out\n",
    "\n",
    "        self.training[\"model\"] = models[\"training\"]\n",
    "        self.experimental[\"model\"] = models[\"experimental\"]\n",
    "        self.validation[\"model\"] = models[\"validation\"]\n",
    "        dict_out = {\"status\": passed, \"stopping_object\": stopping_object, \"pdf_models\": pdf_models}\n",
    "        return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62928bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarizes_values(modeltrain, value_name):\n",
    "    \"\"\"Gives a brief Summary.\"\"\"\n",
    "    \n",
    "    value = getattr(modeltrain, value_name)\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(f\"Keys ({value_name})\", justify=\"left\", width=24)\n",
    "    table.add_column(\"Description\", justify=\"left\", width=24)\n",
    "    table.add_column(\"Value\", justify=\"left\", width=24)\n",
    "    \n",
    "    table.add_row(\"model\", \"Model\", f\"{value['model'] if value['model'] is not None else 'None'}\")\n",
    "    table.add_row(\"expdata\", \"Nb. Experiments\", f\"{len(value['expdata'])}\")\n",
    "    table.add_row(\"output\", \"Nb. Outputs\", f\"{len(value['output'])}\")\n",
    "    table.add_row(\"ndata\", \"Nb. Datapoints\", f\"{value['ndata']}\")\n",
    "    \n",
    "    console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30126352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82983506",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'nodes_per_layer': [15, 10, 8], \n",
    "    'activation_per_layer': ['sigmoid', 'sigmoid', 'linear'], \n",
    "    'initializer': 'glorot_normal', \n",
    "    'optimizer': {'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'clipnorm': 1.0}, \n",
    "    'epochs': 900, 'positivity': {'multiplier': 1.05, 'initial': None, 'threshold': 1e-05}, \n",
    "    'stopping_patience': 0.3, 'layer_type': 'dense', 'dropout': 0.0, 'threshold_chi2': 5.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flav_info = [\n",
    "    {'fl': 'sng', 'trainable': False, 'smallx': [1.094, 1.118], 'largex': [1.46, 3.003]}, \n",
    "    {'fl': 'g', 'trainable': False, 'smallx': [0.8189, 1.044], 'largex': [2.791, 5.697]}, \n",
    "    {'fl': 'v', 'trainable': False, 'smallx': [0.457, 0.7326], 'largex': [1.56, 3.431]}, \n",
    "    {'fl': 'v3', 'trainable': False, 'smallx': [0.1462, 0.4061], 'largex': [1.745, 3.452]}, \n",
    "    {'fl': 'v8', 'trainable': False, 'smallx': [0.5401, 0.7665], 'largex': [1.539, 3.393]}, \n",
    "    {'fl': 't3', 'trainable': False, 'smallx': [-0.4401, 0.9163], 'largex': [1.773, 3.333]}, \n",
    "    {'fl': 't8', 'trainable': False, 'smallx': [0.5852, 0.8537], 'largex': [1.533, 3.436]}, \n",
    "    {'fl': 't15', 'trainable': False, 'smallx': [1.082, 1.142], 'largex': [1.461, 3.1]}\n",
    "]\n",
    "\n",
    "nnseed = [1872583848]\n",
    "fitbasis = 'EVOL'\n",
    "debug = False\n",
    "max_cores = 8\n",
    "model_file = None\n",
    "sum_rules = False\n",
    "paralle_models = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelTraining = ModelTrainer(\n",
    "    toyexpinfo,\n",
    "    toyposdatasets,\n",
    "    toyintegdatasets,\n",
    "    flav_info,\n",
    "    fitbasis,\n",
    "    nnseed,\n",
    "    debug=debug,\n",
    "    kfold_parameters=None,\n",
    "    max_cores=max_cores,\n",
    "    sum_rules=sum_rules,\n",
    "    parallel_models=1,\n",
    ")\n",
    "\n",
    "pdf_gen_and_train_function = ModelTraining.hyperparametrizable\n",
    "\n",
    "ModelTraining.set_hyperopt(False)\n",
    "\n",
    "pdf_gen_and_train_function(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5ee13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf40",
   "language": "python",
   "name": "nnpdf40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
